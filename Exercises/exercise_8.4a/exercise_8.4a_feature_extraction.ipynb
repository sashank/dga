{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3148d23",
   "metadata": {},
   "source": [
    "# Exercise 8.4A: Comprehensive Feature Extraction Pipeline for DGA Detection\n",
    "\n",
    "**Type**: Hands-on Implementation  \n",
    "**Duration**: 3-4 hours  \n",
    "**Difficulty**: Intermediate\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Learning Objectives\n",
    "\n",
    "By completing this notebook, you will:\n",
    "- âœ… Understand Domain Generation Algorithm (DGA) detection principles\n",
    "- âœ… Implement comprehensive feature extraction for domains\n",
    "- âœ… Build production-grade, scalable feature pipelines\n",
    "- âœ… Validate feature quality and discriminative power\n",
    "- âœ… Optimize code for real-time performance (<1ms per domain)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ The Problem: Why DGA Detection Matters\n",
    "\n",
    "**Domain Generation Algorithms (DGAs)** are used by malware to:\n",
    "- Generate thousands of domain names dynamically\n",
    "- Evade blacklists and static domain blocking\n",
    "- Establish command-and-control (C2) communications\n",
    "- Rotate domains to avoid detection\n",
    "\n",
    "**Our Mission**: Build a feature extraction system that can process 100,000+ domains per hour in a Security Operations Center (SOC) environment.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Prerequisites & Setup\n",
    "\n",
    "Before starting, ensure you have:\n",
    "- Intermediate Python programming skills\n",
    "- Understanding of feature engineering concepts\n",
    "- Familiarity with NumPy and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38552db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "Pandas version: 2.3.3\n",
      "NumPy version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries (run this cell first if packages are missing)\n",
    "# Uncomment the line below if needed:\n",
    "# !pip install pandas numpy scipy scikit-learn matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dbf9bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”¬ Part 1: Understanding Domain Features\n",
    "\n",
    "Before implementing our pipeline, let's understand what makes a domain \"suspicious.\"\n",
    "\n",
    "### Feature Categories\n",
    "\n",
    "1. **Lexical Features**: Character-level patterns\n",
    "   - Length, digit ratio, entropy, special characters\n",
    "   \n",
    "2. **Linguistic Features**: Language-like properties\n",
    "   - Pronounceability, vowel-consonant patterns, n-grams\n",
    "   \n",
    "3. **Statistical Features**: Distribution analysis\n",
    "   - Character frequency, bigram entropy, randomness\n",
    "   \n",
    "4. **DNS Features**: Domain structure\n",
    "   - TLD classification, subdomain count, label patterns\n",
    "\n",
    "Let's examine some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68e9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DOMAIN EXAMPLES\n",
      "============================================================\n",
      "\n",
      "LEGITIMATE:\n",
      "  â€¢ google.com\n",
      "  â€¢ facebook.com\n",
      "  â€¢ amazon.co.uk\n",
      "  â€¢ stackoverflow.com\n",
      "\n",
      "DGA CRYPTOLOCKER:\n",
      "  â€¢ acmipywpotq.net\n",
      "  â€¢ bdfkqrxtyuc.com\n",
      "  â€¢ ceglosuvwxz.org\n",
      "\n",
      "DGA CONFICKER:\n",
      "  â€¢ jkxpvqwdza.biz\n",
      "  â€¢ nmsuyzbfhj.info\n",
      "  â€¢ plqtwxzach.ru\n",
      "\n",
      "============================================================\n",
      "Notice the differences:\n",
      "  - Legitimate: readable, meaningful words\n",
      "  - DGA: random-looking, high entropy, less pronounceable\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Example domains for analysis\n",
    "example_domains = {\n",
    "    'legitimate': ['google.com', 'facebook.com', 'amazon.co.uk', 'stackoverflow.com'],\n",
    "    'dga_cryptolocker': ['acmipywpotq.net', 'bdfkqrxtyuc.com', 'ceglosuvwxz.org'],\n",
    "    'dga_conficker': ['jkxpvqwdza.biz', 'nmsuyzbfhj.info', 'plqtwxzach.ru']\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DOMAIN EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for category, domains in example_domains.items():\n",
    "    print(f\"\\n{category.upper().replace('_', ' ')}:\")\n",
    "    for domain in domains:\n",
    "        print(f\"  â€¢ {domain}\")\n",
    "        \n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Notice the differences:\")\n",
    "print(\"  - Legitimate: readable, meaningful words\")\n",
    "print(\"  - DGA: random-looking, high entropy, less pronounceable\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d015615",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ› ï¸ Task 1: Implement Feature Extraction Modules\n",
    "\n",
    "We'll build four modular feature extractors. Each focuses on a specific aspect of domain analysis.\n",
    "\n",
    "### 1.1 Lexical Feature Extractor\n",
    "\n",
    "Lexical features capture character-level patterns and structural properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749a377c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Lexical Feature Extractor\n",
      "\n",
      "Domain: google.com\n",
      "  Length: 6\n",
      "  Entropy: 1.918\n",
      "  Vowel Ratio: 0.500\n",
      "  Digit Ratio: 0.000\n",
      "\n",
      "Domain: xqzpkwjt.net\n",
      "  Length: 8\n",
      "  Entropy: 3.000\n",
      "  Vowel Ratio: 0.000\n",
      "  Digit Ratio: 0.000\n",
      "\n",
      "Domain: amazon123.com\n",
      "  Length: 9\n",
      "  Entropy: 2.948\n",
      "  Vowel Ratio: 0.333\n",
      "  Digit Ratio: 0.333\n",
      "\n",
      "âœ… Lexical extractor working!\n"
     ]
    }
   ],
   "source": [
    "class LexicalFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts lexical (character-level) features from domain names.\n",
    "    \n",
    "    Features:\n",
    "    - Length metrics\n",
    "    - Character composition (digits, vowels, consonants)\n",
    "    - Entropy (randomness measure)\n",
    "    - Special character patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vowels = set('aeiou')\n",
    "        self.consonants = set('bcdfghjklmnpqrstvwxyz')\n",
    "        \n",
    "    def extract(self, domain: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Extract all lexical features from a domain.\n",
    "        \n",
    "        Args:\n",
    "            domain: Domain name (e.g., 'google.com')\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of feature name -> value\n",
    "        \"\"\"\n",
    "        # Extract domain name without TLD for analysis\n",
    "        domain_parts = domain.split('.')\n",
    "        domain_name = domain_parts[0].lower()\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 1. Length Features\n",
    "        features['length'] = len(domain_name)\n",
    "        features['length_total'] = len(domain)\n",
    "        \n",
    "        # 2. Character Composition\n",
    "        features['digit_count'] = sum(c.isdigit() for c in domain_name)\n",
    "        features['digit_ratio'] = features['digit_count'] / max(len(domain_name), 1)\n",
    "        \n",
    "        features['vowel_count'] = sum(c in self.vowels for c in domain_name)\n",
    "        features['vowel_ratio'] = features['vowel_count'] / max(len(domain_name), 1)\n",
    "        \n",
    "        features['consonant_count'] = sum(c in self.consonants for c in domain_name)\n",
    "        features['consonant_ratio'] = features['consonant_count'] / max(len(domain_name), 1)\n",
    "        \n",
    "        # 3. Entropy (Shannon Entropy)\n",
    "        features['entropy'] = self._calculate_entropy(domain_name)\n",
    "        \n",
    "        # 4. Special Characters\n",
    "        features['hyphen_count'] = domain_name.count('-')\n",
    "        features['underscore_count'] = domain_name.count('_')\n",
    "        features['special_char_ratio'] = (features['hyphen_count'] + \n",
    "                                         features['underscore_count']) / max(len(domain_name), 1)\n",
    "        \n",
    "        # 5. Case Patterns (for full domain)\n",
    "        features['uppercase_count'] = sum(c.isupper() for c in domain)\n",
    "        features['uppercase_ratio'] = features['uppercase_count'] / max(len(domain), 1)\n",
    "        \n",
    "        # 6. Consecutive Character Patterns\n",
    "        features['max_consecutive_digits'] = self._max_consecutive_type(domain_name, str.isdigit)\n",
    "        features['max_consecutive_consonants'] = self._max_consecutive_chars(domain_name, self.consonants)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _calculate_entropy(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Shannon entropy.\n",
    "        \n",
    "        Entropy measures randomness:\n",
    "        - Low entropy: Predictable (e.g., 'aaaa' or 'google')\n",
    "        - High entropy: Random (e.g., 'xqzpkwjt')\n",
    "        \n",
    "        Formula: H(X) = -Î£ p(x) * log2(p(x))\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        # Count character frequencies\n",
    "        char_counts = Counter(text)\n",
    "        length = len(text)\n",
    "        \n",
    "        # Calculate probability and entropy\n",
    "        entropy = 0.0\n",
    "        for count in char_counts.values():\n",
    "            probability = count / length\n",
    "            entropy -= probability * math.log2(probability)\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    def _max_consecutive_type(self, text: str, check_func) -> int:\n",
    "        \"\"\"Find maximum consecutive characters matching a condition.\"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "        \n",
    "        max_count = 0\n",
    "        current_count = 0\n",
    "        \n",
    "        for char in text:\n",
    "            if check_func(char):\n",
    "                current_count += 1\n",
    "                max_count = max(max_count, current_count)\n",
    "            else:\n",
    "                current_count = 0\n",
    "        \n",
    "        return max_count\n",
    "    \n",
    "    def _max_consecutive_chars(self, text: str, char_set: set) -> int:\n",
    "        \"\"\"Find maximum consecutive characters from a set.\"\"\"\n",
    "        if not text:\n",
    "            return 0\n",
    "        \n",
    "        max_count = 0\n",
    "        current_count = 0\n",
    "        \n",
    "        for char in text:\n",
    "            if char in char_set:\n",
    "                current_count += 1\n",
    "                max_count = max(max_count, current_count)\n",
    "            else:\n",
    "                current_count = 0\n",
    "        \n",
    "        return max_count\n",
    "\n",
    "# Test the lexical extractor\n",
    "print(\"ðŸ§ª Testing Lexical Feature Extractor\\n\")\n",
    "\n",
    "lexical_extractor = LexicalFeatureExtractor()\n",
    "\n",
    "test_domains = ['google.com', 'xqzpkwjt.net', 'amazon123.com']\n",
    "\n",
    "for domain in test_domains:\n",
    "    features = lexical_extractor.extract(domain)\n",
    "    print(f\"Domain: {domain}\")\n",
    "    print(f\"  Length: {features['length']}\")\n",
    "    print(f\"  Entropy: {features['entropy']:.3f}\")\n",
    "    print(f\"  Vowel Ratio: {features['vowel_ratio']:.3f}\")\n",
    "    print(f\"  Digit Ratio: {features['digit_ratio']:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… Lexical extractor working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c3821",
   "metadata": {},
   "source": [
    "### 1.2 Linguistic Feature Extractor\n",
    "\n",
    "Linguistic features capture language-like properties. DGA domains often lack pronounceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11977c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Linguistic Feature Extractor\n",
      "\n",
      "Domain: google.com\n",
      "  Pronounceability: 0.600\n",
      "  VC Transitions: 3\n",
      "  Common Bigrams: 0\n",
      "  Common Trigrams: 0\n",
      "\n",
      "Domain: xqzpkwjt.net\n",
      "  Pronounceability: 0.000\n",
      "  VC Transitions: 0\n",
      "  Common Bigrams: 0\n",
      "  Common Trigrams: 0\n",
      "\n",
      "Domain: banana.com\n",
      "  Pronounceability: 1.000\n",
      "  VC Transitions: 5\n",
      "  Common Bigrams: 2\n",
      "  Common Trigrams: 0\n",
      "\n",
      "âœ… Linguistic extractor working!\n"
     ]
    }
   ],
   "source": [
    "class LinguisticFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts linguistic features - how 'language-like' is the domain?\n",
    "    \n",
    "    Features:\n",
    "    - Pronounceability score\n",
    "    - Vowel-consonant patterns\n",
    "    - N-gram analysis (bigrams, trigrams)\n",
    "    - Dictionary word presence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vowels = set('aeiou')\n",
    "        \n",
    "        # Common English bigrams (top frequency)\n",
    "        self.common_bigrams = {\n",
    "            'th', 'he', 'in', 'er', 'an', 're', 'on', 'at', 'en', 'nd',\n",
    "            'ti', 'es', 'or', 'te', 'of', 'ed', 'is', 'it', 'al', 'ar'\n",
    "        }\n",
    "        \n",
    "        # Common English trigrams\n",
    "        self.common_trigrams = {\n",
    "            'the', 'and', 'ing', 'ion', 'tio', 'ent', 'ati', 'for', 'her', 'ter'\n",
    "        }\n",
    "    \n",
    "    def extract(self, domain: str) -> Dict[str, float]:\n",
    "        \"\"\"Extract linguistic features.\"\"\"\n",
    "        domain_name = domain.split('.')[0].lower()\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 1. Pronounceability Score\n",
    "        features['pronounceability'] = self._calculate_pronounceability(domain_name)\n",
    "        \n",
    "        # 2. Vowel-Consonant Transitions\n",
    "        features['vowel_consonant_transitions'] = self._count_vc_transitions(domain_name)\n",
    "        features['vc_transition_ratio'] = features['vowel_consonant_transitions'] / max(len(domain_name) - 1, 1)\n",
    "        \n",
    "        # 3. N-gram Analysis\n",
    "        bigrams = self._extract_ngrams(domain_name, 2)\n",
    "        trigrams = self._extract_ngrams(domain_name, 3)\n",
    "        \n",
    "        features['common_bigram_count'] = sum(1 for bg in bigrams if bg in self.common_bigrams)\n",
    "        features['common_bigram_ratio'] = features['common_bigram_count'] / max(len(bigrams), 1)\n",
    "        \n",
    "        features['common_trigram_count'] = sum(1 for tg in trigrams if tg in self.common_trigrams)\n",
    "        features['common_trigram_ratio'] = features['common_trigram_count'] / max(len(trigrams), 1)\n",
    "        \n",
    "        # 4. Unique N-grams (diversity)\n",
    "        features['unique_bigram_ratio'] = len(set(bigrams)) / max(len(bigrams), 1)\n",
    "        features['unique_trigram_ratio'] = len(set(trigrams)) / max(len(trigrams), 1)\n",
    "        \n",
    "        # 5. Repeating Patterns\n",
    "        features['repeating_patterns'] = self._find_repeating_patterns(domain_name)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _calculate_pronounceability(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate pronounceability score (0-1).\n",
    "        \n",
    "        Based on vowel-consonant alternation. Languages typically alternate\n",
    "        between vowels and consonants (e.g., 'banana', 'google').\n",
    "        Random strings have poor alternation (e.g., 'xqzpk').\n",
    "        \"\"\"\n",
    "        if len(text) < 2:\n",
    "            return 0.5\n",
    "        \n",
    "        # Count good transitions (V->C or C->V)\n",
    "        good_transitions = 0\n",
    "        \n",
    "        for i in range(len(text) - 1):\n",
    "            curr_is_vowel = text[i] in self.vowels\n",
    "            next_is_vowel = text[i + 1] in self.vowels\n",
    "            \n",
    "            # Good if alternating\n",
    "            if curr_is_vowel != next_is_vowel:\n",
    "                good_transitions += 1\n",
    "        \n",
    "        return good_transitions / (len(text) - 1)\n",
    "    \n",
    "    def _count_vc_transitions(self, text: str) -> int:\n",
    "        \"\"\"Count vowel-consonant transitions.\"\"\"\n",
    "        if len(text) < 2:\n",
    "            return 0\n",
    "        \n",
    "        transitions = 0\n",
    "        for i in range(len(text) - 1):\n",
    "            curr_is_vowel = text[i] in self.vowels\n",
    "            next_is_vowel = text[i + 1] in self.vowels\n",
    "            \n",
    "            if curr_is_vowel != next_is_vowel:\n",
    "                transitions += 1\n",
    "        \n",
    "        return transitions\n",
    "    \n",
    "    def _extract_ngrams(self, text: str, n: int) -> List[str]:\n",
    "        \"\"\"Extract n-grams from text.\"\"\"\n",
    "        if len(text) < n:\n",
    "            return []\n",
    "        \n",
    "        return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "    \n",
    "    def _find_repeating_patterns(self, text: str) -> int:\n",
    "        \"\"\"Count repeating character patterns.\"\"\"\n",
    "        if len(text) < 2:\n",
    "            return 0\n",
    "        \n",
    "        repeats = 0\n",
    "        for i in range(len(text) - 1):\n",
    "            if text[i] == text[i + 1]:\n",
    "                repeats += 1\n",
    "        \n",
    "        return repeats\n",
    "\n",
    "# Test the linguistic extractor\n",
    "print(\"ðŸ§ª Testing Linguistic Feature Extractor\\n\")\n",
    "\n",
    "linguistic_extractor = LinguisticFeatureExtractor()\n",
    "\n",
    "test_domains = ['google.com', 'xqzpkwjt.net', 'banana.com']\n",
    "\n",
    "for domain in test_domains:\n",
    "    features = linguistic_extractor.extract(domain)\n",
    "    print(f\"Domain: {domain}\")\n",
    "    print(f\"  Pronounceability: {features['pronounceability']:.3f}\")\n",
    "    print(f\"  VC Transitions: {features['vowel_consonant_transitions']}\")\n",
    "    print(f\"  Common Bigrams: {features['common_bigram_count']}\")\n",
    "    print(f\"  Common Trigrams: {features['common_trigram_count']}\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… Linguistic extractor working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b79f79",
   "metadata": {},
   "source": [
    "### 1.3 Statistical Feature Extractor\n",
    "\n",
    "Statistical features analyze the distribution of characters and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "303042b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Statistical Feature Extractor\n",
      "\n",
      "Domain: google.com\n",
      "  Unique Char Ratio: 0.667\n",
      "  Bigram Entropy: 2.322\n",
      "  Randomness Score: 0.704\n",
      "  Char Freq Variance: 0.250\n",
      "\n",
      "Domain: xqzpkwjt.net\n",
      "  Unique Char Ratio: 1.000\n",
      "  Bigram Entropy: 2.807\n",
      "  Randomness Score: 1.000\n",
      "  Char Freq Variance: 0.000\n",
      "\n",
      "Domain: aaabbbccc.com\n",
      "  Unique Char Ratio: 0.333\n",
      "  Bigram Entropy: 2.250\n",
      "  Randomness Score: 0.417\n",
      "  Char Freq Variance: 0.000\n",
      "\n",
      "âœ… Statistical extractor working!\n"
     ]
    }
   ],
   "source": [
    "class StatisticalFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts statistical features based on character distributions.\n",
    "    \n",
    "    Features:\n",
    "    - Character frequency variance\n",
    "    - N-gram entropy\n",
    "    - Randomness tests\n",
    "    - Distribution metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def extract(self, domain: str) -> Dict[str, float]:\n",
    "        \"\"\"Extract statistical features.\"\"\"\n",
    "        domain_name = domain.split('.')[0].lower()\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 1. Character Frequency Analysis\n",
    "        char_freq = Counter(domain_name)\n",
    "        frequencies = list(char_freq.values())\n",
    "        \n",
    "        if frequencies:\n",
    "            features['char_freq_mean'] = np.mean(frequencies)\n",
    "            features['char_freq_std'] = np.std(frequencies)\n",
    "            features['char_freq_variance'] = np.var(frequencies)\n",
    "            features['unique_char_ratio'] = len(char_freq) / max(len(domain_name), 1)\n",
    "        else:\n",
    "            features['char_freq_mean'] = 0\n",
    "            features['char_freq_std'] = 0\n",
    "            features['char_freq_variance'] = 0\n",
    "            features['unique_char_ratio'] = 0\n",
    "        \n",
    "        # 2. N-gram Entropy\n",
    "        bigrams = self._extract_ngrams(domain_name, 2)\n",
    "        features['bigram_entropy'] = self._calculate_ngram_entropy(bigrams)\n",
    "        \n",
    "        trigrams = self._extract_ngrams(domain_name, 3)\n",
    "        features['trigram_entropy'] = self._calculate_ngram_entropy(trigrams)\n",
    "        \n",
    "        # 3. Character Distribution Skewness & Kurtosis\n",
    "        if len(frequencies) > 1:\n",
    "            features['char_freq_skewness'] = stats.skew(frequencies)\n",
    "            features['char_freq_kurtosis'] = stats.kurtosis(frequencies)\n",
    "        else:\n",
    "            features['char_freq_skewness'] = 0\n",
    "            features['char_freq_kurtosis'] = 0\n",
    "        \n",
    "        # 4. Randomness Score (combining multiple metrics)\n",
    "        features['randomness_score'] = self._calculate_randomness(domain_name)\n",
    "        \n",
    "        # 5. Alphabetic Position Statistics\n",
    "        features['avg_char_position'] = self._avg_alphabetic_position(domain_name)\n",
    "        features['char_position_std'] = self._std_alphabetic_position(domain_name)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _extract_ngrams(self, text: str, n: int) -> List[str]:\n",
    "        \"\"\"Extract n-grams.\"\"\"\n",
    "        if len(text) < n:\n",
    "            return []\n",
    "        return [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "    \n",
    "    def _calculate_ngram_entropy(self, ngrams: List[str]) -> float:\n",
    "        \"\"\"Calculate entropy of n-gram distribution.\"\"\"\n",
    "        if not ngrams:\n",
    "            return 0.0\n",
    "        \n",
    "        ngram_counts = Counter(ngrams)\n",
    "        total = len(ngrams)\n",
    "        \n",
    "        entropy = 0.0\n",
    "        for count in ngram_counts.values():\n",
    "            probability = count / total\n",
    "            entropy -= probability * math.log2(probability)\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    def _calculate_randomness(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate overall randomness score (0-1).\n",
    "        \n",
    "        Combines:\n",
    "        - High entropy\n",
    "        - High unique character ratio\n",
    "        - Low pronounceability\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        # Entropy component (normalized)\n",
    "        char_counts = Counter(text)\n",
    "        entropy = 0.0\n",
    "        for count in char_counts.values():\n",
    "            p = count / len(text)\n",
    "            entropy -= p * math.log2(p)\n",
    "        \n",
    "        max_entropy = math.log2(len(text)) if len(text) > 1 else 1\n",
    "        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "        \n",
    "        # Unique character component\n",
    "        uniqueness = len(char_counts) / len(text)\n",
    "        \n",
    "        # Combine\n",
    "        randomness = (normalized_entropy + uniqueness) / 2\n",
    "        \n",
    "        return randomness\n",
    "    \n",
    "    def _avg_alphabetic_position(self, text: str) -> float:\n",
    "        \"\"\"Average position of characters in alphabet (a=1, z=26).\"\"\"\n",
    "        positions = []\n",
    "        for char in text:\n",
    "            if char.isalpha():\n",
    "                positions.append(ord(char.lower()) - ord('a') + 1)\n",
    "        \n",
    "        return np.mean(positions) if positions else 0.0\n",
    "    \n",
    "    def _std_alphabetic_position(self, text: str) -> float:\n",
    "        \"\"\"Standard deviation of character positions in alphabet.\"\"\"\n",
    "        positions = []\n",
    "        for char in text:\n",
    "            if char.isalpha():\n",
    "                positions.append(ord(char.lower()) - ord('a') + 1)\n",
    "        \n",
    "        return np.std(positions) if positions else 0.0\n",
    "\n",
    "# Test the statistical extractor\n",
    "print(\"ðŸ§ª Testing Statistical Feature Extractor\\n\")\n",
    "\n",
    "statistical_extractor = StatisticalFeatureExtractor()\n",
    "\n",
    "test_domains = ['google.com', 'xqzpkwjt.net', 'aaabbbccc.com']\n",
    "\n",
    "for domain in test_domains:\n",
    "    features = statistical_extractor.extract(domain)\n",
    "    print(f\"Domain: {domain}\")\n",
    "    print(f\"  Unique Char Ratio: {features['unique_char_ratio']:.3f}\")\n",
    "    print(f\"  Bigram Entropy: {features['bigram_entropy']:.3f}\")\n",
    "    print(f\"  Randomness Score: {features['randomness_score']:.3f}\")\n",
    "    print(f\"  Char Freq Variance: {features['char_freq_variance']:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… Statistical extractor working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d5e6d",
   "metadata": {},
   "source": [
    "### 1.4 DNS Feature Extractor\n",
    "\n",
    "DNS features capture domain structure and naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fa05b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing DNS Feature Extractor\n",
      "\n",
      "Domain: google.com\n",
      "  TLD Common: 1\n",
      "  TLD Suspicious: 0\n",
      "  Subdomain Count: 0\n",
      "  Total Labels: 2\n",
      "\n",
      "Domain: mail.google.com\n",
      "  TLD Common: 1\n",
      "  TLD Suspicious: 0\n",
      "  Subdomain Count: 1\n",
      "  Total Labels: 3\n",
      "\n",
      "Domain: xqzpk123.tk\n",
      "  TLD Common: 0\n",
      "  TLD Suspicious: 1\n",
      "  Subdomain Count: 0\n",
      "  Total Labels: 2\n",
      "\n",
      "Domain: amazon.co.uk\n",
      "  TLD Common: 0\n",
      "  TLD Suspicious: 0\n",
      "  Subdomain Count: 1\n",
      "  Total Labels: 3\n",
      "\n",
      "âœ… DNS extractor working!\n"
     ]
    }
   ],
   "source": [
    "class DNSFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts DNS-related structural features.\n",
    "    \n",
    "    Features:\n",
    "    - TLD classification\n",
    "    - Subdomain analysis\n",
    "    - Label patterns\n",
    "    - Domain structure\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Common legitimate TLDs\n",
    "        self.common_tlds = {\n",
    "            'com', 'org', 'net', 'edu', 'gov', 'co', 'io', 'info', 'biz'\n",
    "        }\n",
    "        \n",
    "        # Suspicious TLDs (often used in malware campaigns)\n",
    "        self.suspicious_tlds = {\n",
    "            'tk', 'ml', 'ga', 'cf', 'gq', 'pw', 'cc', 'xyz', 'top'\n",
    "        }\n",
    "    \n",
    "    def extract(self, domain: str) -> Dict[str, float]:\n",
    "        \"\"\"Extract DNS structure features.\"\"\"\n",
    "        parts = domain.split('.')\n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # 1. TLD Analysis\n",
    "        if len(parts) >= 2:\n",
    "            tld = parts[-1].lower()\n",
    "            features['tld_is_common'] = 1 if tld in self.common_tlds else 0\n",
    "            features['tld_is_suspicious'] = 1 if tld in self.suspicious_tlds else 0\n",
    "            features['tld_length'] = len(tld)\n",
    "        else:\n",
    "            features['tld_is_common'] = 0\n",
    "            features['tld_is_suspicious'] = 0\n",
    "            features['tld_length'] = 0\n",
    "        \n",
    "        # 2. Subdomain Analysis\n",
    "        features['subdomain_count'] = len(parts) - 2 if len(parts) > 2 else 0\n",
    "        features['has_subdomain'] = 1 if features['subdomain_count'] > 0 else 0\n",
    "        \n",
    "        # 3. Label Analysis\n",
    "        features['total_labels'] = len(parts)\n",
    "        \n",
    "        if parts:\n",
    "            label_lengths = [len(p) for p in parts]\n",
    "            features['avg_label_length'] = np.mean(label_lengths)\n",
    "            features['max_label_length'] = max(label_lengths)\n",
    "            features['min_label_length'] = min(label_lengths)\n",
    "        else:\n",
    "            features['avg_label_length'] = 0\n",
    "            features['max_label_length'] = 0\n",
    "            features['min_label_length'] = 0\n",
    "        \n",
    "        # 4. Domain Name (without TLD) Analysis\n",
    "        if parts:\n",
    "            domain_name = parts[0]\n",
    "            features['domain_name_length'] = len(domain_name)\n",
    "            \n",
    "            # Check for numerical suffix (common in DGA)\n",
    "            features['has_numerical_suffix'] = 1 if domain_name and domain_name[-1].isdigit() else 0\n",
    "        else:\n",
    "            features['domain_name_length'] = 0\n",
    "            features['has_numerical_suffix'] = 0\n",
    "        \n",
    "        # 5. Dot Density\n",
    "        features['dot_count'] = domain.count('.')\n",
    "        features['dot_density'] = domain.count('.') / max(len(domain), 1)\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Test the DNS extractor\n",
    "print(\"ðŸ§ª Testing DNS Feature Extractor\\n\")\n",
    "\n",
    "dns_extractor = DNSFeatureExtractor()\n",
    "\n",
    "test_domains = ['google.com', 'mail.google.com', 'xqzpk123.tk', 'amazon.co.uk']\n",
    "\n",
    "for domain in test_domains:\n",
    "    features = dns_extractor.extract(domain)\n",
    "    print(f\"Domain: {domain}\")\n",
    "    print(f\"  TLD Common: {features['tld_is_common']}\")\n",
    "    print(f\"  TLD Suspicious: {features['tld_is_suspicious']}\")\n",
    "    print(f\"  Subdomain Count: {features['subdomain_count']}\")\n",
    "    print(f\"  Total Labels: {features['total_labels']}\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… DNS extractor working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6711ecd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ—ï¸ Task 2: Build Scalable Pipeline\n",
    "\n",
    "Now let's combine all extractors into a unified, production-ready pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b541dbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Unified Feature Extraction Pipeline\n",
      "\n",
      "======================================================================\n",
      "\n",
      "âœ… Extracted features from 10 domains\n",
      "â±ï¸ Time: 0.02s | Rate: 654 domains/sec\n",
      "ðŸ“Š Features: 50 total\n",
      "\n",
      "======================================================================\n",
      "FEATURE EXTRACTION RESULTS\n",
      "======================================================================\n",
      "\n",
      "Shape: (10, 51)\n",
      "Columns: 51\n",
      "\n",
      "First few rows:\n",
      "         domain  length  length_total  digit_count  digit_ratio  vowel_count  \\\n",
      "0    google.com       6            10            0          0.0            3   \n",
      "1  facebook.com       8            12            0          0.0            4   \n",
      "2    amazon.com       6            10            0          0.0            3   \n",
      "\n",
      "   vowel_ratio  consonant_count  consonant_ratio   entropy  ...  \\\n",
      "0          0.5                3              0.5  1.918296  ...   \n",
      "1          0.5                4              0.5  2.750000  ...   \n",
      "2          0.5                3              0.5  2.251629  ...   \n",
      "\n",
      "   subdomain_count  has_subdomain  total_labels  avg_label_length  \\\n",
      "0                0              0             2               4.5   \n",
      "1                0              0             2               5.5   \n",
      "2                0              0             2               4.5   \n",
      "\n",
      "   max_label_length  min_label_length  domain_name_length  \\\n",
      "0                 6                 3                   6   \n",
      "1                 8                 3                   8   \n",
      "2                 6                 3                   6   \n",
      "\n",
      "   has_numerical_suffix  dot_count  dot_density  \n",
      "0                     0          1     0.100000  \n",
      "1                     0          1     0.083333  \n",
      "2                     0          1     0.100000  \n",
      "\n",
      "[3 rows x 51 columns]\n",
      "\n",
      "======================================================================\n",
      "âœ… Pipeline working successfully!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "class DomainFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Unified feature extraction pipeline for domain names.\n",
    "    \n",
    "    Combines all feature modules:\n",
    "    - Lexical\n",
    "    - Linguistic\n",
    "    - Statistical\n",
    "    - DNS\n",
    "    \n",
    "    Optimized for:\n",
    "    - Batch processing\n",
    "    - Performance (>1000 domains/second)\n",
    "    - Error handling\n",
    "    - Feature scaling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, enable_scaling: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the feature extractor.\n",
    "        \n",
    "        Args:\n",
    "            enable_scaling: Whether to apply feature scaling\n",
    "        \"\"\"\n",
    "        self.lexical_extractor = LexicalFeatureExtractor()\n",
    "        self.linguistic_extractor = LinguisticFeatureExtractor()\n",
    "        self.statistical_extractor = StatisticalFeatureExtractor()\n",
    "        self.dns_extractor = DNSFeatureExtractor()\n",
    "        \n",
    "        self.enable_scaling = enable_scaling\n",
    "        self.scaler = StandardScaler() if enable_scaling else None\n",
    "        self.feature_names = []\n",
    "        self._is_fitted = False\n",
    "    \n",
    "    def extract_single(self, domain: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Extract all features from a single domain.\n",
    "        \n",
    "        Args:\n",
    "            domain: Domain name\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of all features\n",
    "        \"\"\"\n",
    "        try:\n",
    "            features = {}\n",
    "            \n",
    "            # Extract from all modules\n",
    "            features.update(self.lexical_extractor.extract(domain))\n",
    "            features.update(self.linguistic_extractor.extract(domain))\n",
    "            features.update(self.statistical_extractor.extract(domain))\n",
    "            features.update(self.dns_extractor.extract(domain))\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing domain '{domain}': {str(e)}\")\n",
    "            return {}\n",
    "    \n",
    "    def extract_batch(self, domains: List[str], show_progress: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract features from a batch of domains.\n",
    "        \n",
    "        Args:\n",
    "            domains: List of domain names\n",
    "            show_progress: Whether to show progress\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with features for all domains\n",
    "        \"\"\"\n",
    "        features_list = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, domain in enumerate(domains):\n",
    "            features = self.extract_single(domain)\n",
    "            if features:\n",
    "                features['domain'] = domain\n",
    "                features_list.append(features)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if show_progress and (i + 1) % 1000 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = (i + 1) / elapsed\n",
    "                print(f\"Processed {i + 1}/{len(domains)} domains ({rate:.0f} domains/sec)\")\n",
    "        \n",
    "        df = pd.DataFrame(features_list)\n",
    "        \n",
    "        # Move domain column to first position\n",
    "        if 'domain' in df.columns:\n",
    "            cols = ['domain'] + [col for col in df.columns if col != 'domain']\n",
    "            df = df[cols]\n",
    "        \n",
    "        # Store feature names (excluding 'domain')\n",
    "        self.feature_names = [col for col in df.columns if col != 'domain']\n",
    "        \n",
    "        # Apply scaling if enabled\n",
    "        if self.enable_scaling and not self._is_fitted:\n",
    "            self.fit_scaler(df)\n",
    "        \n",
    "        if self.enable_scaling and self._is_fitted:\n",
    "            df = self.transform_features(df)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        rate = len(domains) / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        if show_progress:\n",
    "            print(f\"\\nâœ… Extracted features from {len(df)} domains\")\n",
    "            print(f\"â±ï¸ Time: {elapsed:.2f}s | Rate: {rate:.0f} domains/sec\")\n",
    "            print(f\"ðŸ“Š Features: {len(self.feature_names)} total\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_scaler(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Fit the scaler on feature data.\"\"\"\n",
    "        if self.scaler is None:\n",
    "            return\n",
    "        \n",
    "        feature_cols = [col for col in df.columns if col != 'domain']\n",
    "        self.scaler.fit(df[feature_cols])\n",
    "        self._is_fitted = True\n",
    "        print(\"âœ… Scaler fitted on features\")\n",
    "    \n",
    "    def transform_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply scaling transformation to features.\"\"\"\n",
    "        if self.scaler is None or not self._is_fitted:\n",
    "            return df\n",
    "        \n",
    "        feature_cols = [col for col in df.columns if col != 'domain']\n",
    "        df_scaled = df.copy()\n",
    "        df_scaled[feature_cols] = self.scaler.transform(df[feature_cols])\n",
    "        \n",
    "        return df_scaled\n",
    "    \n",
    "    def get_feature_summary(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate summary statistics for all features.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with extracted features\n",
    "            \n",
    "        Returns:\n",
    "            Summary DataFrame with statistics\n",
    "        \"\"\"\n",
    "        feature_cols = [col for col in df.columns if col != 'domain']\n",
    "        \n",
    "        summary = df[feature_cols].describe().T\n",
    "        summary['missing'] = df[feature_cols].isnull().sum()\n",
    "        summary['missing_pct'] = (summary['missing'] / len(df)) * 100\n",
    "        \n",
    "        return summary.round(3)\n",
    "\n",
    "# Test the unified pipeline\n",
    "print(\"ðŸ§ª Testing Unified Feature Extraction Pipeline\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create test dataset\n",
    "test_domains = [\n",
    "    'google.com', 'facebook.com', 'amazon.com', 'twitter.com',\n",
    "    'xqzpkwjt.net', 'abcdefgh.com', 'hjklmnop.org',\n",
    "    'mail.google.com', 'api.github.com', 'stackoverflow.com'\n",
    "]\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = DomainFeatureExtractor(enable_scaling=False)\n",
    "\n",
    "# Extract features\n",
    "features_df = pipeline.extract_batch(test_domains, show_progress=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE EXTRACTION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nShape: {features_df.shape}\")\n",
    "print(f\"Columns: {len(features_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(features_df.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… Pipeline working successfully!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d627f",
   "metadata": {},
   "source": [
    "### Performance Benchmark\n",
    "\n",
    "Let's test if we meet our performance requirements (>1000 domains/second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd58be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽï¸ PERFORMANCE BENCHMARK\n",
      "======================================================================\n",
      "\n",
      "100 domains:\n",
      "  Time: 0.091s\n",
      "  Rate: 1102 domains/sec âœ… PASS\n",
      "  Avg per domain: 0.91ms\n",
      "\n",
      "500 domains:\n",
      "  Time: 0.437s\n",
      "  Rate: 1144 domains/sec âœ… PASS\n",
      "  Avg per domain: 0.87ms\n",
      "\n",
      "1,000 domains:\n",
      "  Time: 0.864s\n",
      "  Rate: 1158 domains/sec âœ… PASS\n",
      "  Avg per domain: 0.86ms\n",
      "\n",
      "5,000 domains:\n",
      "  Time: 4.324s\n",
      "  Rate: 1156 domains/sec âœ… PASS\n",
      "  Avg per domain: 0.86ms\n",
      "\n",
      "======================================================================\n",
      "TARGET: >1000 domains/second, <1ms per domain\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic test data for performance testing\n",
    "print(\"ðŸŽï¸ PERFORMANCE BENCHMARK\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create diverse test domains\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_random_domain(length_range=(5, 15)):\n",
    "    \"\"\"Generate random domain for testing.\"\"\"\n",
    "    length = np.random.randint(length_range[0], length_range[1])\n",
    "    chars = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    domain_name = ''.join(np.random.choice(list(chars)) for _ in range(length))\n",
    "    tld = np.random.choice(['com', 'net', 'org', 'io', 'co'])\n",
    "    return f\"{domain_name}.{tld}\"\n",
    "\n",
    "# Generate test sets of different sizes\n",
    "test_sizes = [100, 500, 1000, 5000]\n",
    "\n",
    "for size in test_sizes:\n",
    "    test_domains = [generate_random_domain() for _ in range(size)]\n",
    "    \n",
    "    pipeline = DomainFeatureExtractor(enable_scaling=False)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    features_df = pipeline.extract_batch(test_domains, show_progress=False)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    rate = size / elapsed\n",
    "    avg_time_per_domain = (elapsed / size) * 1000  # in milliseconds\n",
    "    \n",
    "    status = \"âœ… PASS\" if rate > 1000 else \"âš ï¸ SLOW\"\n",
    "    \n",
    "    print(f\"\\n{size:,} domains:\")\n",
    "    print(f\"  Time: {elapsed:.3f}s\")\n",
    "    print(f\"  Rate: {rate:.0f} domains/sec {status}\")\n",
    "    print(f\"  Avg per domain: {avg_time_per_domain:.2f}ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TARGET: >1000 domains/second, <1ms per domain\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b24dc9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š Task 3: Feature Validation & Analysis\n",
    "\n",
    "Now let's validate our features and understand their discriminative power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7346f8",
   "metadata": {},
   "source": [
    "### 3.1 Create Sample Dataset\n",
    "\n",
    "We'll create a labeled dataset with legitimate and DGA domains for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97e00af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Creating Sample Dataset\n",
      "======================================================================\n",
      "\n",
      "âœ… Extracted features from 60 domains\n",
      "â±ï¸ Time: 0.08s | Rate: 758 domains/sec\n",
      "ðŸ“Š Features: 50 total\n",
      "\n",
      "âœ… Dataset created:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m features_df[\u001b[33m'\u001b[39m\u001b[33mlabel_name\u001b[39m\u001b[33m'\u001b[39m] = features_df[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].map({\u001b[32m0\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mLegitimate\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mDGA\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… Dataset created:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Legitimate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m==\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m domains\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   DGA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(labels\u001b[38;5;250m \u001b[39m==\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m domains\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Total Features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pipeline.feature_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Create a labeled dataset for validation\n",
    "print(\"ðŸ“¦ Creating Sample Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Legitimate domains (real world examples)\n",
    "legitimate_domains = [\n",
    "    'google.com', 'facebook.com', 'amazon.com', 'twitter.com', 'linkedin.com',\n",
    "    'microsoft.com', 'apple.com', 'github.com', 'stackoverflow.com', 'reddit.com',\n",
    "    'wikipedia.org', 'youtube.com', 'netflix.com', 'instagram.com', 'pinterest.com',\n",
    "    'ebay.com', 'paypal.com', 'dropbox.com', 'adobe.com', 'salesforce.com',\n",
    "    'oracle.com', 'ibm.com', 'intel.com', 'cisco.com', 'nvidia.com',\n",
    "    'airbnb.com', 'uber.com', 'spotify.com', 'zoom.us', 'slack.com'\n",
    "]\n",
    "\n",
    "# Simulate DGA domains (random-looking strings)\n",
    "def generate_dga_like_domain(length_range=(8, 20)):\n",
    "    \"\"\"Generate DGA-like domain with high entropy.\"\"\"\n",
    "    length = np.random.randint(length_range[0], length_range[1])\n",
    "    \n",
    "    # Use less common character combinations\n",
    "    chars = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    weights = np.random.dirichlet(np.ones(len(chars)))  # Random distribution\n",
    "    \n",
    "    domain_name = ''.join(np.random.choice(list(chars), p=weights) for _ in range(length))\n",
    "    tld = np.random.choice(['net', 'com', 'org', 'info', 'biz'])\n",
    "    \n",
    "    return f\"{domain_name}.{tld}\"\n",
    "\n",
    "np.random.seed(42)\n",
    "dga_domains = [generate_dga_like_domain() for _ in range(30)]\n",
    "\n",
    "# Create labeled dataset\n",
    "all_domains = legitimate_domains + dga_domains\n",
    "labels = [0] * len(legitimate_domains) + [1] * len(dga_domains)  # 0=legitimate, 1=DGA\n",
    "\n",
    "# Extract features\n",
    "pipeline = DomainFeatureExtractor(enable_scaling=False)\n",
    "features_df = pipeline.extract_batch(all_domains, show_progress=True)\n",
    "\n",
    "# Add labels\n",
    "features_df['label'] = labels\n",
    "features_df['label_name'] = features_df['label'].map({0: 'Legitimate', 1: 'DGA'})\n",
    "\n",
    "print(f\"\\nâœ… Dataset created:\")\n",
    "print(f\"   Legitimate: {labels.count(0)} domains\")\n",
    "print(f\"   DGA: {labels.count(1)} domains\")\n",
    "print(f\"   Total Features: {len(pipeline.feature_names)}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample domains:\")\n",
    "print(features_df[['domain', 'label_name']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91410b9a",
   "metadata": {},
   "source": [
    "### 3.2 Feature Distribution Analysis\n",
    "\n",
    "Let's visualize how features differ between legitimate and DGA domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db044cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key features\n",
    "print(\"ðŸ“Š Analyzing Feature Distributions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select key features for visualization\n",
    "key_features = [\n",
    "    'length', 'entropy', 'vowel_ratio', 'pronounceability',\n",
    "    'randomness_score', 'bigram_entropy', 'unique_char_ratio'\n",
    "]\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(key_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot distributions for each class\n",
    "    legitimate_data = features_df[features_df['label'] == 0][feature]\n",
    "    dga_data = features_df[features_df['label'] == 1][feature]\n",
    "    \n",
    "    ax.hist(legitimate_data, alpha=0.6, label='Legitimate', bins=15, color='green')\n",
    "    ax.hist(dga_data, alpha=0.6, label='DGA', bins=15, color='red')\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(len(key_features), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Feature distribution plot saved as 'feature_distributions.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0e0a0",
   "metadata": {},
   "source": [
    "### 3.3 Feature Correlation Analysis\n",
    "\n",
    "Understanding feature correlations helps remove redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd72bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"ðŸ”— Feature Correlation Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "feature_cols = [col for col in features_df.columns \n",
    "                if col not in ['domain', 'label', 'label_name']]\n",
    "\n",
    "correlation_matrix = features_df[feature_cols].corr()\n",
    "\n",
    "# Plot correlation heatmap (top correlations only for readability)\n",
    "plt.figure(figsize=(14, 12))\n",
    "\n",
    "# Select subset of features for clearer visualization\n",
    "feature_subset = key_features + ['digit_ratio', 'vc_transition_ratio', 'tld_is_common']\n",
    "corr_subset = features_df[feature_subset].corr()\n",
    "\n",
    "sns.heatmap(corr_subset, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_correlation.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features\n",
    "threshold = 0.8\n",
    "high_corr_pairs = []\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "            high_corr_pairs.append({\n",
    "                'feature1': correlation_matrix.columns[i],\n",
    "                'feature2': correlation_matrix.columns[j],\n",
    "                'correlation': correlation_matrix.iloc[i, j]\n",
    "            })\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(f\"\\nâš ï¸ Found {len(high_corr_pairs)} highly correlated pairs (|r| > {threshold}):\")\n",
    "    for pair in high_corr_pairs[:5]:  # Show top 5\n",
    "        print(f\"  â€¢ {pair['feature1']} â†” {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "else:\n",
    "    print(f\"\\nâœ… No highly correlated features found (threshold: {threshold})\")\n",
    "\n",
    "print(\"\\nâœ… Correlation analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060c7cdf",
   "metadata": {},
   "source": [
    "### 3.4 Feature Importance Analysis\n",
    "\n",
    "Which features are most discriminative for DGA detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using mutual information\n",
    "print(\"â­ Feature Importance Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate mutual information scores\n",
    "X = features_df[feature_cols]\n",
    "y = features_df['label']\n",
    "\n",
    "# Handle any infinite or NaN values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(0)\n",
    "\n",
    "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': mi_scores\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"\\nðŸ† Top 15 Most Discriminative Features:\")\n",
    "print(\"=\" * 70)\n",
    "for idx, row in feature_importance.head(15).iterrows():\n",
    "    bar = 'â–ˆ' * int(row['importance'] * 50)\n",
    "    print(f\"{row['feature']:30s} | {bar} {row['importance']:.4f}\")\n",
    "\n",
    "# Visualize top features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_n = 20\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Mutual Information Score', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title(f'Top {top_n} Most Important Features for DGA Detection', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Feature importance plot saved as 'feature_importance.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d349757",
   "metadata": {},
   "source": [
    "### 3.5 Class Separability Analysis\n",
    "\n",
    "Let's see how well our features separate legitimate from DGA domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db98fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class separability analysis\n",
    "print(\"ðŸŽ¯ Class Separability Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate per-feature statistics for each class\n",
    "separability_stats = []\n",
    "\n",
    "for feature in feature_cols:\n",
    "    legitimate_values = features_df[features_df['label'] == 0][feature]\n",
    "    dga_values = features_df[features_df['label'] == 1][feature]\n",
    "    \n",
    "    # Calculate means and standard deviations\n",
    "    legit_mean = legitimate_values.mean()\n",
    "    dga_mean = dga_values.mean()\n",
    "    \n",
    "    legit_std = legitimate_values.std()\n",
    "    dga_std = dga_values.std()\n",
    "    \n",
    "    # Calculate separation (difference in means relative to pooled std)\n",
    "    pooled_std = np.sqrt((legit_std**2 + dga_std**2) / 2)\n",
    "    separation = abs(legit_mean - dga_mean) / (pooled_std + 1e-10)\n",
    "    \n",
    "    separability_stats.append({\n",
    "        'feature': feature,\n",
    "        'legitimate_mean': legit_mean,\n",
    "        'dga_mean': dga_mean,\n",
    "        'separation': separation\n",
    "    })\n",
    "\n",
    "separability_df = pd.DataFrame(separability_stats).sort_values('separation', ascending=False)\n",
    "\n",
    "print(\"\\nðŸ† Top 10 Features with Best Class Separation:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Feature':<30} {'Legit Mean':>12} {'DGA Mean':>12} {'Separation':>12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, row in separability_df.head(10).iterrows():\n",
    "    print(f\"{row['feature']:<30} {row['legitimate_mean']:>12.3f} \"\n",
    "          f\"{row['dga_mean']:>12.3f} {row['separation']:>12.3f}\")\n",
    "\n",
    "# Visualize separation for top features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "top_separable = separability_df.head(6)['feature'].tolist()\n",
    "\n",
    "for idx, feature in enumerate(top_separable):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    legitimate_data = features_df[features_df['label'] == 0][feature]\n",
    "    dga_data = features_df[features_df['label'] == 1][feature]\n",
    "    \n",
    "    ax.boxplot([legitimate_data, dga_data], labels=['Legitimate', 'DGA'])\n",
    "    ax.set_title(feature.replace('_', ' ').title(), fontweight='bold')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_separability.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Class separability plot saved as 'class_separability.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961e631",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ›¡ï¸ Task 4: Production Readiness\n",
    "\n",
    "Let's add production-grade features: error handling, logging, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9302ae2",
   "metadata": {},
   "source": [
    "### 4.1 Enhanced Pipeline with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "class ProductionDomainFeatureExtractor(DomainFeatureExtractor):\n",
    "    \"\"\"\n",
    "    Production-ready feature extractor with:\n",
    "    - Comprehensive error handling\n",
    "    - Logging\n",
    "    - Input validation\n",
    "    - Metrics tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, enable_scaling: bool = False, log_errors: bool = True):\n",
    "        super().__init__(enable_scaling)\n",
    "        self.log_errors = log_errors\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        # Metrics\n",
    "        self.total_processed = 0\n",
    "        self.total_errors = 0\n",
    "        self.error_domains = []\n",
    "    \n",
    "    def validate_domain(self, domain: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Validate domain name format.\n",
    "        \n",
    "        Returns:\n",
    "            (is_valid, error_message)\n",
    "        \"\"\"\n",
    "        if not domain or not isinstance(domain, str):\n",
    "            return False, \"Domain must be a non-empty string\"\n",
    "        \n",
    "        if len(domain) > 253:  # RFC 1035\n",
    "            return False, \"Domain exceeds maximum length (253 characters)\"\n",
    "        \n",
    "        if '..' in domain:\n",
    "            return False, \"Domain contains consecutive dots\"\n",
    "        \n",
    "        if domain.startswith('.') or domain.endswith('.'):\n",
    "            return False, \"Domain starts or ends with dot\"\n",
    "        \n",
    "        # Basic pattern check\n",
    "        if not re.match(r'^[a-zA-Z0-9.-]+$', domain):\n",
    "            return False, \"Domain contains invalid characters\"\n",
    "        \n",
    "        return True, None\n",
    "    \n",
    "    def extract_single(self, domain: str) -> Dict[str, float]:\n",
    "        \"\"\"Extract features with validation and error handling.\"\"\"\n",
    "        self.total_processed += 1\n",
    "        \n",
    "        # Validate input\n",
    "        is_valid, error_msg = self.validate_domain(domain)\n",
    "        if not is_valid:\n",
    "            self.total_errors += 1\n",
    "            self.error_domains.append({'domain': domain, 'error': error_msg})\n",
    "            \n",
    "            if self.log_errors:\n",
    "                self.logger.warning(f\"Invalid domain '{domain}': {error_msg}\")\n",
    "            \n",
    "            return {}\n",
    "        \n",
    "        # Extract features with error handling\n",
    "        try:\n",
    "            features = super().extract_single(domain)\n",
    "            \n",
    "            # Validate feature values (check for NaN, inf)\n",
    "            for key, value in features.items():\n",
    "                if not isinstance(value, (int, float)):\n",
    "                    continue\n",
    "                    \n",
    "                if math.isnan(value) or math.isinf(value):\n",
    "                    features[key] = 0.0  # Replace invalid values\n",
    "                    if self.log_errors:\n",
    "                        self.logger.warning(f\"Invalid value for {key} in domain '{domain}': {value}\")\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.total_errors += 1\n",
    "            self.error_domains.append({'domain': domain, 'error': str(e)})\n",
    "            \n",
    "            if self.log_errors:\n",
    "                self.logger.error(f\"Error extracting features from '{domain}': {str(e)}\")\n",
    "            \n",
    "            return {}\n",
    "    \n",
    "    def get_metrics(self) -> Dict:\n",
    "        \"\"\"Get processing metrics.\"\"\"\n",
    "        return {\n",
    "            'total_processed': self.total_processed,\n",
    "            'total_errors': self.total_errors,\n",
    "            'error_rate': self.total_errors / max(self.total_processed, 1),\n",
    "            'success_rate': 1 - (self.total_errors / max(self.total_processed, 1))\n",
    "        }\n",
    "    \n",
    "    def get_error_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Get detailed error report.\"\"\"\n",
    "        if not self.error_domains:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return pd.DataFrame(self.error_domains)\n",
    "\n",
    "# Test production pipeline\n",
    "print(\"ðŸ­ Testing Production Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test with various inputs (including invalid ones)\n",
    "test_cases = [\n",
    "    'google.com',          # Valid\n",
    "    'facebook.com',        # Valid\n",
    "    '',                    # Invalid: empty\n",
    "    'invalid..domain.com', # Invalid: consecutive dots\n",
    "    '.startsdot.com',      # Invalid: starts with dot\n",
    "    'a' * 300 + '.com',    # Invalid: too long\n",
    "    'valid123.org',        # Valid\n",
    "    'special!@#.com',      # Invalid: special chars\n",
    "]\n",
    "\n",
    "prod_pipeline = ProductionDomainFeatureExtractor(enable_scaling=False, log_errors=True)\n",
    "results = prod_pipeline.extract_batch(test_cases, show_progress=False)\n",
    "\n",
    "# Show metrics\n",
    "print(\"\\nðŸ“Š Processing Metrics:\")\n",
    "metrics = prod_pipeline.get_metrics()\n",
    "for key, value in metrics.items():\n",
    "    if 'rate' in key:\n",
    "        print(f\"  {key}: {value*100:.1f}%\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Show error report\n",
    "if prod_pipeline.total_errors > 0:\n",
    "    print(\"\\nâš ï¸ Error Report:\")\n",
    "    print(prod_pipeline.get_error_report())\n",
    "\n",
    "print(\"\\nâœ… Production pipeline tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42cb71a",
   "metadata": {},
   "source": [
    "### 4.2 Unit Tests\n",
    "\n",
    "Professional code needs tests. Let's write some basic test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897862ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lexical_features():\n",
    "    \"\"\"Test lexical feature extraction.\"\"\"\n",
    "    extractor = LexicalFeatureExtractor()\n",
    "    \n",
    "    # Test basic domain\n",
    "    features = extractor.extract('google.com')\n",
    "    assert 'length' in features\n",
    "    assert 'entropy' in features\n",
    "    assert features['length'] == 6  # 'google'\n",
    "    \n",
    "    # Test domain with digits\n",
    "    features = extractor.extract('test123.com')\n",
    "    assert features['digit_count'] == 3\n",
    "    assert features['digit_ratio'] > 0\n",
    "    \n",
    "    print(\"âœ… Lexical features test passed\")\n",
    "\n",
    "def test_linguistic_features():\n",
    "    \"\"\"Test linguistic feature extraction.\"\"\"\n",
    "    extractor = LinguisticFeatureExtractor()\n",
    "    \n",
    "    # Test pronounceable domain\n",
    "    features = extractor.extract('banana.com')\n",
    "    assert 'pronounceability' in features\n",
    "    assert features['pronounceability'] > 0.5  # Should be pronounceable\n",
    "    \n",
    "    # Test random domain\n",
    "    features = extractor.extract('xqzpk.com')\n",
    "    assert features['pronounceability'] < 0.5  # Should be less pronounceable\n",
    "    \n",
    "    print(\"âœ… Linguistic features test passed\")\n",
    "\n",
    "def test_statistical_features():\n",
    "    \"\"\"Test statistical feature extraction.\"\"\"\n",
    "    extractor = StatisticalFeatureExtractor()\n",
    "    \n",
    "    features = extractor.extract('google.com')\n",
    "    assert 'bigram_entropy' in features\n",
    "    assert 'randomness_score' in features\n",
    "    assert 'unique_char_ratio' in features\n",
    "    \n",
    "    print(\"âœ… Statistical features test passed\")\n",
    "\n",
    "def test_dns_features():\n",
    "    \"\"\"Test DNS feature extraction.\"\"\"\n",
    "    extractor = DNSFeatureExtractor()\n",
    "    \n",
    "    # Test with subdomain\n",
    "    features = extractor.extract('mail.google.com')\n",
    "    assert features['subdomain_count'] == 1\n",
    "    assert features['total_labels'] == 3\n",
    "    \n",
    "    # Test without subdomain\n",
    "    features = extractor.extract('google.com')\n",
    "    assert features['subdomain_count'] == 0\n",
    "    \n",
    "    print(\"âœ… DNS features test passed\")\n",
    "\n",
    "def test_pipeline_integration():\n",
    "    \"\"\"Test full pipeline.\"\"\"\n",
    "    pipeline = DomainFeatureExtractor()\n",
    "    \n",
    "    domains = ['google.com', 'facebook.com', 'twitter.com']\n",
    "    result = pipeline.extract_batch(domains, show_progress=False)\n",
    "    \n",
    "    assert len(result) == 3\n",
    "    assert 'domain' in result.columns\n",
    "    assert len(result.columns) > 10  # Should have many features\n",
    "    \n",
    "    print(\"âœ… Pipeline integration test passed\")\n",
    "\n",
    "# Run all tests\n",
    "print(\"ðŸ§ª Running Unit Tests\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    test_lexical_features()\n",
    "    test_linguistic_features()\n",
    "    test_statistical_features()\n",
    "    test_dns_features()\n",
    "    test_pipeline_integration()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"âœ… ALL TESTS PASSED\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "except AssertionError as e:\n",
    "    print(f\"\\nâŒ TEST FAILED: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ ERROR: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93fe8c9",
   "metadata": {},
   "source": [
    "### 4.3 Export Features and Metadata\n",
    "\n",
    "For production use, we need to export features and their descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_feature_metadata(pipeline: DomainFeatureExtractor, \n",
    "                           output_file: str = 'feature_metadata.csv'):\n",
    "    \"\"\"\n",
    "    Export feature metadata (names, descriptions, types).\n",
    "    \n",
    "    Args:\n",
    "        pipeline: Fitted feature extractor\n",
    "        output_file: Output CSV file path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define feature metadata\n",
    "    metadata = []\n",
    "    \n",
    "    # Lexical features\n",
    "    lexical_features = {\n",
    "        'length': 'Length of domain name (without TLD)',\n",
    "        'length_total': 'Total length including TLD',\n",
    "        'digit_count': 'Number of digit characters',\n",
    "        'digit_ratio': 'Ratio of digits to total characters',\n",
    "        'vowel_count': 'Number of vowel characters',\n",
    "        'vowel_ratio': 'Ratio of vowels to total characters',\n",
    "        'consonant_count': 'Number of consonant characters',\n",
    "        'consonant_ratio': 'Ratio of consonants to total characters',\n",
    "        'entropy': 'Shannon entropy (randomness measure)',\n",
    "        'hyphen_count': 'Number of hyphens',\n",
    "        'underscore_count': 'Number of underscores',\n",
    "        'special_char_ratio': 'Ratio of special characters',\n",
    "        'uppercase_count': 'Number of uppercase letters',\n",
    "        'uppercase_ratio': 'Ratio of uppercase letters',\n",
    "        'max_consecutive_digits': 'Maximum consecutive digits',\n",
    "        'max_consecutive_consonants': 'Maximum consecutive consonants'\n",
    "    }\n",
    "    \n",
    "    # Linguistic features\n",
    "    linguistic_features = {\n",
    "        'pronounceability': 'How pronounceable the domain is (0-1)',\n",
    "        'vowel_consonant_transitions': 'Number of vowel-consonant transitions',\n",
    "        'vc_transition_ratio': 'Ratio of V-C transitions',\n",
    "        'common_bigram_count': 'Number of common English bigrams',\n",
    "        'common_bigram_ratio': 'Ratio of common bigrams',\n",
    "        'common_trigram_count': 'Number of common English trigrams',\n",
    "        'common_trigram_ratio': 'Ratio of common trigrams',\n",
    "        'unique_bigram_ratio': 'Ratio of unique bigrams',\n",
    "        'unique_trigram_ratio': 'Ratio of unique trigrams',\n",
    "        'repeating_patterns': 'Number of repeating character patterns'\n",
    "    }\n",
    "    \n",
    "    # Statistical features\n",
    "    statistical_features = {\n",
    "        'char_freq_mean': 'Mean character frequency',\n",
    "        'char_freq_std': 'Standard deviation of character frequency',\n",
    "        'char_freq_variance': 'Variance of character frequency',\n",
    "        'unique_char_ratio': 'Ratio of unique characters',\n",
    "        'bigram_entropy': 'Entropy of bigram distribution',\n",
    "        'trigram_entropy': 'Entropy of trigram distribution',\n",
    "        'char_freq_skewness': 'Skewness of character frequency distribution',\n",
    "        'char_freq_kurtosis': 'Kurtosis of character frequency distribution',\n",
    "        'randomness_score': 'Overall randomness score (0-1)',\n",
    "        'avg_char_position': 'Average alphabetic position of characters',\n",
    "        'char_position_std': 'Std dev of character positions'\n",
    "    }\n",
    "    \n",
    "    # DNS features\n",
    "    dns_features = {\n",
    "        'tld_is_common': 'Whether TLD is common (1/0)',\n",
    "        'tld_is_suspicious': 'Whether TLD is suspicious (1/0)',\n",
    "        'tld_length': 'Length of TLD',\n",
    "        'subdomain_count': 'Number of subdomains',\n",
    "        'has_subdomain': 'Whether domain has subdomain (1/0)',\n",
    "        'total_labels': 'Total number of DNS labels',\n",
    "        'avg_label_length': 'Average length of DNS labels',\n",
    "        'max_label_length': 'Maximum label length',\n",
    "        'min_label_length': 'Minimum label length',\n",
    "        'domain_name_length': 'Length of primary domain name',\n",
    "        'has_numerical_suffix': 'Whether domain ends with digit (1/0)',\n",
    "        'dot_count': 'Number of dots in full domain',\n",
    "        'dot_density': 'Ratio of dots to total length'\n",
    "    }\n",
    "    \n",
    "    # Combine all features\n",
    "    all_features = {\n",
    "        **lexical_features,\n",
    "        **linguistic_features,\n",
    "        **statistical_features,\n",
    "        **dns_features\n",
    "    }\n",
    "    \n",
    "    # Create metadata rows\n",
    "    for feature_name, description in all_features.items():\n",
    "        # Determine category\n",
    "        if feature_name in lexical_features:\n",
    "            category = 'Lexical'\n",
    "        elif feature_name in linguistic_features:\n",
    "            category = 'Linguistic'\n",
    "        elif feature_name in statistical_features:\n",
    "            category = 'Statistical'\n",
    "        else:\n",
    "            category = 'DNS'\n",
    "        \n",
    "        metadata.append({\n",
    "            'feature_name': feature_name,\n",
    "            'category': category,\n",
    "            'description': description,\n",
    "            'type': 'numeric'\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and export\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    metadata_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"âœ… Feature metadata exported to '{output_file}'\")\n",
    "    print(f\"   Total features: {len(metadata_df)}\")\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "# Export metadata\n",
    "print(\"ðŸ“„ Exporting Feature Metadata\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "pipeline = DomainFeatureExtractor()\n",
    "metadata_df = export_feature_metadata(pipeline)\n",
    "\n",
    "print(\"\\nSample metadata:\")\n",
    "print(metadata_df.head(10))\n",
    "\n",
    "print(\"\\nFeatures by category:\")\n",
    "print(metadata_df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6175d44c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Summary & Key Takeaways\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "âœ… **Implemented 4 Feature Extraction Modules**:\n",
    "- Lexical: Character-level patterns (length, entropy, composition)\n",
    "- Linguistic: Language-like properties (pronounceability, n-grams)\n",
    "- Statistical: Distribution analysis (entropy, randomness)\n",
    "- DNS: Domain structure (TLD, subdomains, labels)\n",
    "\n",
    "âœ… **Built Production-Ready Pipeline**:\n",
    "- Unified `DomainFeatureExtractor` class\n",
    "- Batch processing (>1000 domains/second)\n",
    "- Error handling and validation\n",
    "- Feature scaling and normalization\n",
    "\n",
    "âœ… **Validated Features**:\n",
    "- Extracted 50+ features per domain\n",
    "- Analyzed feature distributions\n",
    "- Identified discriminative features\n",
    "- Examined class separability\n",
    "\n",
    "âœ… **Production Readiness**:\n",
    "- Comprehensive error handling\n",
    "- Logging and metrics\n",
    "- Unit tests\n",
    "- Feature metadata export\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Most Discriminative Features** (for DGA detection):\n",
    "   - Entropy (Shannon entropy)\n",
    "   - Pronounceability\n",
    "   - Randomness score\n",
    "   - Bigram/trigram entropy\n",
    "   - Vowel-consonant patterns\n",
    "\n",
    "2. **Performance Achieved**:\n",
    "   - Processing rate: >1000 domains/second âœ…\n",
    "   - Latency: <1ms per domain âœ…\n",
    "   - Feature count: 50+ âœ…\n",
    "\n",
    "3. **Class Separability**:\n",
    "   - Legitimate domains: Low entropy, high pronounceability, common patterns\n",
    "   - DGA domains: High entropy, low pronounceability, random patterns\n",
    "\n",
    "---\n",
    "\n",
    "### Next Steps for Students\n",
    "\n",
    "1. **Enhance Features**:\n",
    "   - Add temporal features (registration date, age)\n",
    "   - Include WHOIS information\n",
    "   - Incorporate DNS query patterns\n",
    "\n",
    "2. **Build ML Models**:\n",
    "   - Train classifiers (Random Forest, XGBoost, Neural Networks)\n",
    "   - Evaluate with proper metrics (Precision@k, TPR@1%FPR)\n",
    "   - Handle class imbalance\n",
    "\n",
    "3. **Deploy to Production**:\n",
    "   - Integrate with SIEM/security tools\n",
    "   - Implement real-time processing\n",
    "   - Monitor for concept drift\n",
    "   - Set up retraining pipeline\n",
    "\n",
    "4. **Advanced Topics**:\n",
    "   - Multi-class DGA family classification\n",
    "   - Adversarial robustness\n",
    "   - Explainable predictions (SHAP, LIME)\n",
    "\n",
    "---\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- **Datasets**: DGArchive, OSINT feeds, Alexa Top 1M\n",
    "- **Papers**: \"Beyond Blacklists\" (Antonakakis et al.), DGA taxonomies\n",
    "- **Tools**: Scikit-learn, XGBoost, TensorFlow\n",
    "- **Frameworks**: MITRE ATT&CK (T1568), Cyber Kill Chain\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Remember\n",
    "\n",
    "- **Security Context**: Features should align with threat models\n",
    "- **Operational Constraints**: False positives break SOC workflows\n",
    "- **Concept Drift**: Attackers evolve; models must retrain\n",
    "- **Explainability**: Analysts need to understand predictions\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You've built a production-grade feature extraction pipeline for DGA detection.** ðŸŽ‰\n",
    "\n",
    "This foundation is critical for building effective machine learning-based security systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20815efa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Exercise Deliverables Checklist\n",
    "\n",
    "Before submitting, ensure you have:\n",
    "\n",
    "- [ ] âœ… Implemented all 4 feature extraction modules\n",
    "- [ ] âœ… Created unified `DomainFeatureExtractor` pipeline\n",
    "- [ ] âœ… Achieved >1000 domains/second throughput\n",
    "- [ ] âœ… Generated feature distribution visualizations\n",
    "- [ ] âœ… Completed correlation analysis\n",
    "- [ ] âœ… Analyzed feature importance\n",
    "- [ ] âœ… Added error handling and validation\n",
    "- [ ] âœ… Written unit tests\n",
    "- [ ] âœ… Exported feature metadata\n",
    "- [ ] âœ… Documented code with comments\n",
    "\n",
    "### Files to Submit\n",
    "\n",
    "1. **This notebook** (`exercise_8.4a_feature_extraction.ipynb`) - fully executed\n",
    "2. **Python module** (optional): `domain_features.py` with classes\n",
    "3. **Visualizations**: All generated PNG files\n",
    "4. **Metadata**: `feature_metadata.csv`\n",
    "5. **README**: Document your implementation approach\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your implementation!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
